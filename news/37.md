# 何恺明香港中文大学讲座回顾 | 科研经历、大模型、AI For Science

最近“AI大牛”何恺明在香港中文大学参加讲座时提到“科研中95%的时间是令人沮丧的”这一观点时，立马引起无数科研人共鸣 。其实整场讲座不止有关科研经历，还有关大模型，AI for Science等诸多方向的思考。本文为大家找来了完整版视频，在不改变原意的基础上，就大家最为感兴趣的问答环节进行了梳理。

****何恺明完整版问题解答****

![](https://pic.imgdb.cn/item/65f11f479f345e8d03bc139c.png)

**大模型的未来：数据效益是个问题**

**​Q：​**您刚刚（演讲）展示的图片，呈现了深度网络加深时，性能先上升后下降的趋势。

![](https://pic.imgdb.cn/item/65f11f519f345e8d03bc38ed.png)

起初人们可能误认为是过拟合导致的，就增加数据量，问题确实得到了改善。但又发现当神经网络真的非常深入时，性能还是会再次下降。而你的研究揭示了这其实与某种优化并不是最佳解决方案有关，基本上涉及三大要素：数据量、网络深度、模型复杂度及其优化方式。

考虑到现如今的大模型数据量比以前要大得多，那么您认为可能存在哪些局限性？或者接下来应该如何应对数据模型复杂性和优化带来的挑战？

**​何恺明：​**通常，我们认为增加网络的深度和宽度是提高神经网络模型性能的方法。而在机器学习中，拟合与泛化之间存在权衡，也就是说要实现适当的拟合并减少过拟合。

目前要想减少过拟合、提高泛化，最有效的方法就是增加数据量。

虽然大量数据的拟合和记忆仍是一个挑战，但大模型其实有足够的能力做到这一点，事实也证明增加数据量是减少过拟合的最佳解决方案。

然而展望未来，​**数据带来的效益是否会降低是个问题**​。

比如说，语言数据不是凭空产生的，而是由人类创造出来的。你在写一些新的文本时，是带有想分享信息、创作新知识等某种目的的。所以文本数据中的信息可能比许多其它形式的数据中的信息都要更丰富。

而一张新的照片可能并不会增加太多新的信息。尽管它看起来可能包含更多的信息，但实际上你每天用手机拍摄的内容也许只是你的食物或是自拍。

所以不同类型的数据所含信息量不同，继续增加数据的回报可能会有所减少。我认为这将是未来的一个开放性的问题。

**Q:** 您提到如今深度学习像是残差学习已广泛应用于多个领域，例如AlphaGo和AlphaFold等。

回顾一二十年前，研究人员会专注于研究每一个具体的小问题，手动设计各种算法。但现如今，大部分问题都是由更通用的模型来学习解决的。

那么您认为未来的发展趋势是会出现一个能够处理大多数任务的大型预训练模型，而我们只需对其进行微调来适应特定的任务？还是说仍然有一些问题需要手动设计或用更具体的领域知识来解决？

**​何恺明：​**我认为这两个方向将会同步发展。

在自然语言处理中，预训练模型基本上是默认方法。但在计算机视觉领域，情况稍有不同，因为人们还没有提出一个好的想法来开发所谓的视觉基础模型。

这或许是因为视觉任务更为多样化，而且更重要的是，语言是人类智慧的产物，而像素则来自于自然，这是语言和图像之间的本质区别。

展望未来，我们希望神经网络能够处理更多的问题，比如科学问题、蛋白质、分子、材料，甚至是在数学、化学和物理中推导方程。

我们希望有通用基础模型来解决大部分问题，但同时也期望有专家模型在特定领域推动技术进步。

**​Q：​**您认为AI距离能够进行抽象数学研究还有多远？如果我们继续沿着现在的方向前进，我们最终会到达那个目标吗？或者您认为两者之间存在一个根本的鸿沟吗？

**​何恺明：​**坦白说我并不是这个方向的专家，但是可能有两种方法可以实现。一种是只是训练一个大模型，然后希望这个模型能够自行解决问题，但我不认为这是一个有前景的方向。

另一个方向是，如果你为大模型配备了一些代码等能力，比如ChatGPT代码解释器。也就是说允许语言模型编写代码，这些代码可以进行一些计算或是符号操作，然后那种计算可以给模型提供反馈。这样的话，模型可以决定下一步要做什么。我认为这是一个更有前景的方法。

我们也可以考虑这样一个情境，如果我们回到牛顿时代，我们有那个时代的所有文本和数据，并且在那个时代训练了一个大语言模型，有一天这个模型是否可以告诉我们牛顿定律？

如果我们能做到这一点，那么如果我们只给它今天的数据，它会告诉我们一些还不知道的定律吗？我认为这是非常高水平的人工智能。这是一个终极目标。

**​Q：​**您如何看待AI在艺术和人文学科中的未来应用？

**​何恺明：​**我不是这方面的专家。看起来艺术和人文真的是人类大脑中非常特殊的领域。我认为问题应该是，人类大脑与AI之间的根本区别是什么。

如果有一天我们可以物理地复制我们的大脑，但我们称其为机器，那么那个大脑所做的事情可以称之为艺术或人文吗？还是我们应该继续称其为人工输出呢？我认为这是一个哲学问题，更像是一个科幻问题。

### **未来三年研究重点：视觉自监督学习​**

**​Q：​**您未来三年的研究重点是什么？

**​何恺明：​**基本上，我会做所有事情。如今自然语言处理取得了很大成功，因为人们可以在语言数据上进行自监督学习，但计算机视觉尚未完全解决这一问题。

所以，我一直努力让计算机视觉复制这种成功，也就是说我想让​**视觉自监督学习也取得成功**​。

那么，成功的定义是什么呢？我希望看到与语言模型相同的​**规模效应**​：只是增加模型的大小、数据量，就能看到视觉模型具有更强大的能力。

不幸的是，这种情况尚未实现。如今，语言模型非常成功，视觉加上语言也非常成功。但对于计算机视觉来说，这还没有实现。所以，这将是我接下来三年，甚至可能是我整个职业生涯的研究重点。

**​Q：​**您提到想要探索图像领域的自监督。在自然语言处理中，句子词汇中已经包含了一些语义知识，但在图像中像素只是像RGB这样，实际上不包含任何语义知识，它们来自自然。

所以我想知道，是否有只来自于图像本身的监督？我也想知道如何定义这种自监督？

**​何恺明：​**我认为这是语言与视觉之间的根本区别，这也是我们想要解决但迄今尚未能解决的主要问题。我认为表示学习中最困难的部分是如何在语言问题中进行抽象和压缩，这部分工作人类已经完成了。

图像这方面，来自传感器的输入比语言更加自然，因此模型需要自己来完成压缩和抽象的工作，这仍然是一个未解决的问题。

另一方面，我也认为仅从像素或图像、视频中进行自监督学习是不够的。比如动物可以看到这个世界，但动物也会从这个世界中获得其它反馈。所以它们可以采取行动，可以为了生存寻找食物、逃离捕食。所以它们有很多其它形式的信号、监督或从环境中获得的奖励，并不仅仅是视觉。

然后，我认为我们​**现在的视觉系统缺乏来自环境的反馈**​，这可能是视觉自监督学习的下一个研究主题。

### **选择课题的标准：好奇心和热情**

**​Q：​**如何找到一个好的研究课题，可以发表为CVPR的那种？

**​何恺明：​**我认为发表不应该是最终的目标。发表应该是研究成果的起点，但不是终点。你的论文生命周期从发表的那一刻开始，我希望你能有这样的预期。

但我还是会回答如何选择研究课题，并希望你能将其发表。

我认为选择课题最重要的标准是你​**对问题的好奇心和热情**​。

好奇心是人类推进科学进步、探索未知问题的根本原因。我不关心是否发表，我只关心为什么这个问题会这样表现，我只关心我如何解决这个问题。如果我发现了答案，那么可能就有了一篇论文；如果我没能解决，那么也许只是有一篇小幅进展的论文，但那都不重要。

**好奇心和热情才应该是我们研究生涯的重心。**

**​Q：​**您在研究中是如何保持好奇心和热情的？对我来说，如果我发现实验中出现了错误，我必须重新进行所有实验，那真的很崩溃。

**​何恺明：​**我认为研究本就充满了挫折、失败和沮丧。实际上，它包含了你能想到的所有负面词汇，这就是事实。如果你没有经历过这些，那意味着你并没有进行最好的研究。

我的生活就是这样，我有大约95%的时间都很失望，然后我会花5%的时间完成那篇论文，接着进入下一个循环，不断经历沮丧、挫败和焦虑，直到下一项工作完成。享受那5%的时光，如此反复。

### **“AI将成为几乎所有事情的基础工具”**

**​Q：​**我听说您打算研究AI for Science，我对此非常感兴趣。比如说，各学科领域的人都学习AI，然后用这些模型进行一些研究；计算机科学领域的人也与其他科学领域的人合作发表论文。您对此有什么看法呢？

**​何恺明：​**我相信AI会成为几乎所有领域的基础工具。回想约四五十年前，那时几乎没有计算机科学系，你可能需要在专门的计算机科学机构里学习一些有关计算机科学的知识。

但现在想想，基本上每一个学科都与某种计算、计算机程序、模拟、数据分析有关。因此，计算机科学现在实际上几乎是每一个学科、每一个领域的工具。

所以，我预测在接下来的十年或是二十年内，AI将是下一代计算机科学，AI将成为几乎所有事情的基础工具。也许你不需要拥有一个有关AI的学位，也不需要进入一个专门的AI机构来学习有关AI的知识，但你会在你的科学问题中用AI发现新的模式、新的行为、新的现象。

我非常期待这一切的发生，这是我的目标，也是我对AI for Science的期望。

**​Q：​**您刚刚提到了这方面可能会产生的一些具体的应用。但是对于某些领域来说，数据量可能较小，数据质量可能也很低，这种情况您怎么看？

**​何恺明：​**数据量的大小都是相对的。\*\*比如图像数据集，按照一二十年前的标准看现在的数据集可能是庞大的，但按今天的标准看它们相对较小。

我认为数据量的大小和相关的算法是相辅相成的，它们以一种螺旋式的方式相互促进。

也就是说，如果你有一定量的数据，你就会为它们开发算法。而当你发现你的算法可以从更多的数据中受益时，你可能会开始收集更多的数据，然后再根据新数据改进你的算法，如此往复。

所以，我认为这既是数据问题，也是算法问题。

**​Q：​**我看到ResNet的关键在于最大化地保持信号，我对此很感兴趣。我正在研究构建光子神经网络，发现它与模拟计算非常吻合，我们应该最大限度地保持信号强度，我认为这是很有创意的，残差学习在模拟计算中将具有巨大的潜力。您对此有什么看法？

**​何恺明：​**我不确定我是否正确地理解了你的问题。我的评论是这样的，当今的人工神经网络最初是受到生物神经网络的启发，但随后这两个方向开始发散。

人工神经网络是专门为某些应用或数据集而设计的，有的可能不具有生物学起源，像残差连接就是这样的。

但有趣的是，实际上还有许多并行的研究是关于映射人脑或动物脑中的连接模式。相关研究有时被称为“连接学”之类的术语。

人们在那些人类或动物的神经网络中发现了与当今最先进的人工神经网络非常相似的模式。这些模式包括长距离跳过连接、循环连接和其他类型的反馈连接。

所以我认为人工智能与认知科学或脑科学可以相互受益。人脑中的发现可以启发我们的AI设计。但另一方面，AI网络中的成功实践也可以启发科学家更好地解释我们的大脑。

### **用疑问解答AI模型可解释性问题**

**​Q：​**我的问题是关于AI模型的可解释性。我发现一些AI模型表现得非常好，在某些指标上可以超越人类。然而，我们如何解释AI模型的整体行为呢？我们是否可以对AI模型进行准确的预测，以及我们的AI模型是否真的可以变得非常可靠？我想知道您如何看待这个问题。

**​何恺明：​**我想问你一个问题，当你乘坐出租车时，为什么你会信任一个人类司机？这位司机一般对你来说是个陌生人，你并不了解他，你只知道他是个人类。

你会信任他是因为你觉得他的大脑是可以解释的？还是因为你认为一个经过良好培训、有丰富实践经验的人类司机在实际操作中大概率会做得很好？

我并不需要你的答案，这是我的疑问。人们也问过同样的问题。为什么我们信任飞机？是因为我们有足够的物理定律或数学推导可以确保飞机在空中飞行，还是因为飞机已经在空中被测试了数百万次？

所以我相信，可解释性是一个非常好的属性，我真心鼓励大家去追求它。但另一方面，我们需要认识到，我们系统的成功大部分也是基于实证来推动或验证的。

视频链接：

\*[https://cutv.cpr.cuhk.edu.hk/detail/1572?t=dr-kaiming-he-2023-future-science-prize-laureates-lecture\*](https://cutv.cpr.cuhk.edu.hk/detail/1572?t=dr-kaiming-he-2023-future-science-prize-laureates-lecture*)

**\*本文转载自公众号：量子位；将门投创**

